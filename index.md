I am currently a PhD student in [National Key Laboratory of Fundamental Science on Synthetic Vision](https://vs.scu.edu.cn/),[Sichuan University](https://www.scu.edu.cn/), supervised by [Prof. Ziliang Feng](https://vs.scu.edu.cn/info/1062/1368.htm). I received master's degree from [Xihua University](https://www.xhu.edu.cn/), under the supervision of [Prof. Zhisheng Gao](https://teacher.xhu.edu.cn/jsj/gzs/list.psp).

I am currently a member at [Intelligent Policing Key Laboratory of Sichuan Province](https://ipklsc.scpolicec.edu.cn/index.htm), [Sichuan Police College](https://www.scpolicec.edu.cn/), Chengdu, China. My research focuses on **low-level vision** ,**computer vision** and **model-based image processing**. My current research is image fusion. 

**Current research:**
+ **Low-level vision:** Multi-modal Image Restoration,Multi-modal visual information fusion,image deblurring, image super-resolution;
+ **Computer Vision:** semantic segmentation,Person Re-identification,concealed scene understanding,RGB-T Tracking,Infrared Small Target Detection;
+ **Multimedia Forensics and Security:** image manipulation detection,DeepFake and Face Forgery detection,facial retouching detection and Restoration;
+ **Machine Learning:** sparse signal processing,Convolutional Sparse Representation;
+ **Multimodal Machine Learning:** Multimodal Fusion.


**Email:**  
+ chengfangzhang@scpolicec.edu.cn
+ zcf1838725417@163.com
+ cfzhang2022@gmail.com
[Wechat](../images/wechatimage.jpg)/[CSDN](https://blog.csdn.net/qq_15951093)/[Zhihu](https://www.zhihu.com/people/a-fang-77-73)/[AMiner](https://www.aminer.cn/profile/54489c1adabfae8575916ab3)


---
# Publications

## Journal papers

<div class="papers-container papers-selected">

<div class="publication media paperhi">
   <img src="./images/journal/16-journal-dcd.png" height="120" width="200" class="papericon">
   <div class="media-body">
      <b>16. An unsupervised multi-focus image fusion method via dual-channel convolutional network and discriminator</b><br>
      Lixing Fang, Xiangxiang Wang, Junli Zhao, Zhenkuan Pan, <strong><b>Hui Li*</b></strong>, Yi Li* <br/>
      Computer Vision and Image Understanding (<b>CVIU</b>), Volume: 103, July 2024, 104029. <br/>
      [<a href="https://doi.org/10.1016/j.cviu.2024.104029">paper</a>][arxiv][code]
   </div>
</div>  

<div class="publication media paperhi">
   <img src="./images/journal/14-journal-crossfuse.png" height="120" width="200" class="papericon">
   <div class="media-body">
	   <b>15. CrossFuse: A Novel Cross Attention Mechanism based Infrared and Visible Image Fusion Approach</b><br>
   	<strong><b>Hui Li*</b></strong>, Xiao-Jun Wu <br/>
	   Information Fusion (<b>InfFus</b>), Volume: 103, March 2024, 102147. <br/>
   	[<a href="https://doi.org/10.1016/j.inffus.2023.102147">paper</a>][arxiv][<a href="https://github.com/hli1221/CrossFuse">code</a>]
   </div>
</div>  

<div class="publication media paperhi">
   <img src="./images/journal/15-journal-guidefuse.png" height="120" width="200" class="papericon">
   <div class="media-body">
      <b>14. GuideFuse: A novel guided auto-encoder fusion network for infrared and visible images</b><br>
      Zeyang Zhang, <strong><b>Hui Li</b></strong>, Tianyang Xu, Xiao-Jun Wu*, Yu Fu <br/>
      IEEE Transactions on Instrumentation and Measurement (<b>TIM</b>), 2023, Volume: 73, 5011211. <br/>
      [<a href="https://doi.org/10.1109/TIM.2023.3306537">paper</a>][arxiv][<a href="https://github.com/Yukarizz/GuideFuse">code</a>]
   </div>
</div>  

<div class="publication media paperhi">
   <img src="./images/journal/13-journal-depf.png" height="120" width="200" class="papericon">
   <div class="media-body">
	   <b>13. DePF: A Novel Fusion Approach based on Decomposition Pooling for Infrared and Visible Images</b><br>
   	<strong><b>Hui Li*</b></strong>, Yongbiao Xiao, Chunyang Cheng, Zhongwei Shen, Xiaoning Song <br/>
	   IEEE Transactions on Instrumentation and Measurement (<b>TIM</b>), 2023, Volume: 72, 5031014. <br/>
   	[<a href="https://ieeexplore.ieee.org/document/10288477">paper</a>][<a href="https://arxiv.org/abs/2305.17376">arxiv</a>][<a href="https://github.com/draymondbiao/DePF">code</a>]
   </div>
</div>  

<div class="publication media paperhi">
   <img src="./images/journal/12-journal-sfpfusion.png" height="120" width="200" class="papericon">
   <div class="media-body">
	   <b>12. SFPFusion: An Improved Vision Transformer Combining Super Feature Attention and Wavelet-Guided Pooling for Infrared and Visible Images Fusion</b><br>
   	<strong><b>Hui Li*</b></strong>, Yongbiao Xiao, Chunyang Cheng, Xiaoning Song <br/>
	   <b>Sensors</b>, 2023, volume: 23, number: 18, 2023: 7870 <br/>
   	[<a href="https://www.mdpi.com/1424-8220/23/18/7870#">paper</a>][arxiv][<a href="https://github.com/hli1221/SFPFusion">code</a>]
   </div>
</div>  

<div class="publication media paperhi">
   <img src="./images/journal/11-journal-dfat.jpg" height="120" width="200" class="papericon">
   <div class="media-body">
	   <b>11. Exploring fusion strategies for accurate RGBT visual object tracking</b><br>
   	Zhangyong Tang, Tianyang Xu, <strong><b>Hui Li</b></strong>, Xiao-Jun Wu*, Xue-Feng Zhu, Josef Kittler <br/>
	   Information Fusion (<b>InfFus</b>), Volume: 99, November 2023, 101881. <br/>
   	[<a href="https://doi.org/10.1016/j.inffus.2023.101881">paper</a>][<a href="https://arxiv.org/abs/2201.08673">arxiv</a>][<a href="https://github.com/Zhangyong-Tang/DFAT">code</a>]
   </div>
</div>  

<div class="publication media paperhi">
   <img src="./images/journal/10-journal-lrrnet.png" height="120" width="200" class="papericon">
   <div class="media-body">
	   <b>10. LRRNet: A novel representation learning guided fusion framework for infrared and visible images</b><br>
   	<strong><b>Hui Li</b></strong>, Tianyang Xu, Xiao-Jun Wu*, Jiwen Lu, Josef Kittler <br/>
	   IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), Volume: 45, Issue: 9, pp. 11040-11052, April 2023 <br/>
   	[<a href="https://doi.org/10.1109/TPAMI.2023.3268209">paper</a>][<a href="https://arxiv.org/abs/2304.05172">arxiv</a>][<a href="https://github.com/hli1221/imagefusion-LRRNet">code</a>]
   </div>
</div>  


<div class="publication media paperhi">
   <img src="./images/journal/9-journal-mote.png" height="120" width="200" class="papericon">
   <div class="media-body">
	   <b>9. I Know How You Move: Explicit Motion Estimation for Human Action Recognition</b><br>
   	Zhongwei Shen, Xiao-Jun Wu*, <strong><b>Hui Li</b></strong>, Tianyang Xu, Cong Wu <br/>
	   IEEE Transactions on Multimedia (<b>TMM</b>), 2022, Early Access.  <br/>
   	[<a href="https://ieeexplore.ieee.org/abstract/document/9907887">paper</a>][<a href="https://github.com/hli1221/MOTion-Estimator-MOTE-">code</a>]
   </div>
</div>  


<div class="publication media paperhi">
   <img src="./images/journal/8-journal-swinfuse.png" height="120" width="200" class="papericon">
   <div class="media-body">
	   <b>8. SwinFuse: A Residual Swin Transformer Fusion Network for Infrared and Visible Images</b><br>
   	Zhishe Wang*, Yanlin Chen, Wenyu Shao, <strong><b>Hui Li</b></strong>, Lei Zhang <br/>
	   IEEE Transactions on Instrumentation and Measurement (<b>TIM</b>), 2022, vol: 71.  <br/>
   	[<a href="https://ieeexplore.ieee.org/document/9832006">paper</a>][<a href="https://github.com/Zhishe-Wang/SwinFuse">code</a>]
   </div>
</div> 


<div class="publication media paperhi">
   <img src="./images/journal/7-journal-glnr.png" height="120" width="200" class="papericon">
   <div class="media-body">
	   <b>7. Generalized n-Dimensional Rigid Registration: Theory and Applications</b><br>
   	Jin Wu, Miaomiao Wang, Hassen Fourati, <strong><b>Hui Li</b></strong>, Yilong Zhu, Chengxi Zhang, Yi Jiang, Xiangcheng Hu, Ming Liu* <br/>
	   IEEE Transactions on Cybernetics (<b>TCYB</b>), Volume: 53, Issue: 2, February 2023.  <br/>
   	[<a href="https://ieeexplore.ieee.org/abstract/document/9768182">paper</a>][<a href="https://github.com/zarathustr/GLnR">code</a>][<a href="https://youtu.be/BwfjQ9ZAyl4">video</a>]
   </div>
</div> 


<div class="publication media paperhi">
   <img src="./images/journal/6-journal-rfnnest.png" height="120" width="200" class="papericon">
   <div class="media-body">
	   <b>6. RFN-Nest: An end-to-end residual fusion network for infrared and visible images</b><br>
   	<strong><b>Hui Li</b></strong>, Xiao-Jun Wu* , Josef Kittler <br/>
	   Information Fusion (<b>InfFus</b>), Volume: 73, Pages: 72-86, September 2021. <br/>
	   (<font color=red>Highly Cited Paper</font>) <br/>
   	[<a href="https://www.sciencedirect.com/science/article/pii/S1566253521000440">paper</a>][<a href="https://arxiv.org/abs/2103.04286">arxiv</a>][<a href="https://github.com/hli1221/imagefusion-rfn-nest">code</a>][<a href="https://www.researchgate.net/publication/350485612_sup-rfn-v2pdf">Supplementary Material</a>]
   </div>
</div> 


<div class="publication media paperhi">
   <img src="./images/journal/5-journal-umfa.png" height="120" width="200" class="papericon">
   <div class="media-body">
	   <b>5. UMFA: a photorealistic style transfer method based on U-Net and multi-layer feature aggregation</b><br>
   	Dongyu Rao, Xiao-Jun Wu*, <strong><b>Hui Li</b></strong>, Josef Kittler, Tianyang Xu <br/>
	   Journal of Electronic Imaging (<b>JEI</b>), Volume: 30, Issue: 5, pp. 053013, September 2021. <br/>
   	[<a href="https://doi.org/10.1117/1.JEI.30.5.053013">paper</a>][<a href="https://arxiv.org/abs/2108.06113">arxiv</a>][<a href="https://github.com/dongyuya/UMFA">code</a>]
   </div>
</div> 


<div class="publication media paperhi">
   <img src="./images/journal/4-journal-nestfuse.png" height="120" width="200" class="papericon">
   <div class="media-body">
	   <b>4. NestFuse: An Infrared and Visible Image Fusion Architecture based on Nest Connection and Spatial/Channel Attention Models</b><br>
   	<strong><b>Hui Li</b></strong>, Xiao-Jun Wu* , Tariq S. Durrani <br/>
	   IEEE Transactions on Instrumentation and Measurement (<b>TIM</b>), Volume: 69, Issue: 12, pp. 9645â?9656, Dec. 2020. <br/>
	   (<font color=red>Highly Cited Paper</font>) <br/>
   	[<a href="https://ieeexplore.ieee.org/document/9127964">paper</a>][<a href="https://arxiv.org/abs/2007.00328">arxiv</a>][<a href="https://github.com/hli1221/imagefusion-nestfuse">code</a>]
   </div>
</div> 


<div class="publication media paperhi">
   <img src="./images/journal/3-journal-mdlatlrr.png" height="120" width="200" class="papericon">
   <div class="media-body">
	   <b>3. MDLatLRR: A novel decomposition method for infrared and visible image fusion</b><br>
   	<strong><b>Hui Li</b></strong>, Xiao-Jun Wu* , Josef Kittler <br/>
	   IEEE Transactions on Image Processing (<b>TIP</b>), Volume: 29, pp. 4733-4746, February, 2020. <br/>
	   (<font color=red>Highly Cited Paper</font>) <br/>
   	[<a href="https://ieeexplore.ieee.org/document/9018389">paper</a>][<a href="https://arxiv.org/abs/1811.02291">arxiv</a>][<a href="https://github.com/hli1221/imagefusion_mdlatlrr">code</a>]
   </div>
</div> 


<div class="publication media paperhi">
   <img src="./images/journal/2-journal-zca.png" height="120" width="200" class="papericon">
   <div class="media-body">
	   <b>2. Infrared and Visible Image Fusion with ResNet and zero-phase component analysis</b><br>
   	<strong><b>Hui Li</b></strong>, Xiao-Jun Wu* , Tariq S. Durrani <br/>
	   Journal of Infrared Physics & Technology (<b>JIPT</b>), Volume 102, November 2019, 103039. <br/>
	   (<font color=red>Highly Cited Paper</font>) <br/>
   	[<a href="https://www.sciencedirect.com/science/article/pii/S1350449519301525">paper</a>][<a href="https://arxiv.org/abs/1806.07119">arxiv</a>][<a href="https://github.com/hli1221/imagefusion_resnet50">code</a>]
   </div>
</div> 


<div class="publication media paperhi">
   <img src="./images/journal/1-journal-densefuse.png" height="120" width="200" class="papericon">
   <div class="media-body">
	   <b>1. DenseFuse: A Fusion Approach to Infrared and Visible Images</b><br>
   	<strong><b>Hui Li</b></strong>, Xiao-Jun Wu* <br/>
	   IEEE Transactions on Image Processing (<b>TIP</b>), Volume: 28, Issue: 5, pp. 2614â?2623, May. 2019. <br/>
	   (<font color=red>Highly Cited Paper</font>) <br/>
   	[<a href="https://ieeexplore.ieee.org/document/8580578">paper</a>][<a href="https://arxiv.org/abs/1804.08361">arxiv</a>][<a href="https://github.com/hli1221/imagefusion_densefuse">code</a>]
   </div>
</div> 


</div>


## Conference papers

<div class="papers-container papers-selected">

<div class="publication media paperhi">
   <img src="./images/conf/8-conf-le2fusion.png" height="120" width="200" class="papericon">
   <div class="media-body">
	   <b>8. LE2Fusion: A novel local edge enhancement module for infrared and visible image fusion</b><br>
   	Yongbiao Xiao, <strong><b>Hui Li*</b></strong>, Chunyang Cheng, Xiaoning Song <br/>
	   International Conference on Image and Graphics (<b>ICIG 2023</b>), 29 October 2023, Lecture Notes in Computer Science, vol 14355. Springer, Cham. <br/>
   	[<a href="https://link.springer.com/chapter/10.1007/978-3-031-46305-1_24">paper</a>][<a href="https://arxiv.org/abs/2305.17374">arxiv</a>][<a href="https://github.com/hli1221/LE2Fusion">code</a>]
   </div>
</div>  

<div class="publication media paperhi">
   <img src="./images/conf/0-conf.png" height="120" width="200" class="papericon">
   <div class="media-body">
	   <b>7. D2-LRR A Medical Image Fusion Method based on Dual-decomposed MDLatLRR</b><br>
   	Xu Song, Tianyu Shen, <strong><b>Hui Li*</b></strong>, Xiao-Jun Wu <br/>
	   International Conference on Machine Vision, Image Processing & Imaging Technology (<b>MVIPIT 2023</b>), 24 September 2023, Hangzhou, Zhejiang, China. <br/>
   	[paper][arxiv][<a href="https://github.com/songxujay/MDLatLRRv2">code</a>]
   </div>
</div>  


<div class="publication media paperhi">
   <img src="./images/conf/0-conf.png" height="120" width="200" class="papericon">
   <div class="media-body">
	   <b>6. Res2NetFuse: A Fusion Method for Infrared and Visible Images</b><br>
   	Xu Song, Yongbiao Xiao, <strong><b>Hui Li*</b></strong>, Xiao-Jun Wu, Jun Sun, Valsile Palade <br/>
	   International Conference on Machine Vision, Image Processing & Imaging Technology (<b>MVIPIT 2023</b>), 24 September 2023, Hangzhou, Zhejiang, China. <br/>
   	[paper][<a href="https://arxiv.org/abs/2112.14540">arxiv</a>][<a href="https://github.com/songxujay/Res2NetFuse">code</a>]
   </div>
</div>

<div class="publication media paperhi">
   <img src="./images/conf/5-conf-mscfuse.png" height="120" width="200" class="papericon">
   <div class="media-body">
	   <b>5. MSC-Fuse: An Unsupervised Multi-scale Convolutional Fusion Framework for Infrared and Visible Image</b><br>
   	Guo-Yang Chen, Xiao-Jun Wu* , <strong><b>Hui Li</b></strong>, Tian-Yang Xu <br/>
	   International Conference on Image and Graphics (<b>ICIG 2021</b>), 30 September 2021, Lecture Notes in Computer Science, vol 12888. Springer, Cham. <br/>
   	[<a href="https://link.springer.com/chapter/10.1007/978-3-030-87355-4_4">paper</a>][<a href="https://github.com/cgyfocus/icig_mscfuse">code</a>]
   </div>
</div>

<div class="publication media paperhi">
   <img src="./images/conf/4-conf-subspace.png" height="120" width="200" class="papericon">
   <div class="media-body">
	   <b>4. Subspace Clustering via Joint Unsupervised Feature Selection</b><br>
   	Wenhua Dong, Xiao-Jun Wu* , <strong><b>Hui Li</b></strong>, Zhen-Hua Feng, Josef Kittler <br/>
	  	IEEE International Conference on Pattern Recognition (<b>ICPR 2020</b>), 2021, Page(s): 3892 - 3898. <br/>
   	[<a href="https://ieeexplore.ieee.org/document/9413101">paper</a>][code]
   </div>
</div>

<div class="publication media paperhi">
   <img src="./images/conf/3-conf-msdnet.png" height="120" width="200" class="papericon">
   <div class="media-body">
	   <b>3. MSDNet for Medical Image Fusion</b><br>
   	Xu Song, Xiao-Jun Wu* , <strong><b>Hui Li</b></strong> <br/>
	 	International Conference on Image and Graphics (<b>ICIG 2019</b>), 28 November 2019, Lecture Notes in Computer Science, vol 11902. Springer, Cham.  <br/>
   	[<a href="https://link.springer.com/chapter/10.1007/978-3-030-34110-7_24">paper</a>][<a href="https://github.com/songxujay/MSDNet-for-Medical-Image-Fusion">code</a>]
   </div>
</div>

<div class="publication media paperhi">
   <img src="./images/conf/2-conf-vggml.png" height="120" width="200" class="papericon">
   <div class="media-body">
	   <b>2. Infrared and Visible Image Fusion using a Deep Learning Framework</b><br>
   	<strong><b>Hui Li</b></strong>, Xiao-Jun Wu* , Josef Kittler <br/>
	   IEEE International Conference on Pattern Recognition (<b>ICPR 2018</b>), 2018, Page(s):2705 - 2710. <br/>
   	[<a href="https://ieeexplore.ieee.org/document/8546006">paper</a>][<a href="https://arxiv.org/abs/1804.06992">arxiv</a>][<a href="https://github.com/hli1221/imagefusion_deeplearning">code</a>]
   </div>
</div> 

<div class="publication media paperhi">
   <img src="./images/conf/1-conf-dllrr.png" height="120" width="200" class="papericon">
   <div class="media-body">
	   <b>1. Multi-focus Image Fusion Using Dictionary Learning and Low-Rank Representation</b><br>
   	<strong><b>Hui Li</b></strong>, Xiao-Jun Wu* <br/>
	   International Conference on Image and Graphics* (<b>ICIG 2017</b>), Springer, Cham, 2017: 675 - 686. <br/>
   	[<a href="https://link.springer.com/chapter/10.1007/978-3-319-71607-7_59">paper</a>][<a href="https://arxiv.org/abs/1804.08355">arxiv</a>][<a href="https://github.com/hli1221/imagefusion_dllrr">code</a>]
   </div>
</div> 


</div>

---
# Preprint

<div class="papers-container papers-selected">

<div class="publication media paperhi">
   <img src="./images/preprint/4-preprint-s4fusion.png" height="120" width="200" class="papericon">
   <div class="media-body">
      <b>4. S4Fusion: Saliency-aware Selective State Space Model for Infrared Visible Image Fusion</b><br>
      Haolong Ma, <strong><b>Hui Li*</b></strong>, Chunyang Cheng, Gaoang Wang, Xiaoning Song, Xiaojun Wu <br/>
      arXiv 2024 <br/>
      [<a href="https://arxiv.org/abs/2405.20881">arxiv</a>][<a href="https://github.com/zipper112/S4Fusion">code</a>]
   </div>
</div>  

<div class="publication media paperhi">
   <img src="./images/preprint/3-preprint-comofusion.png" height="120" width="200" class="papericon">
   <div class="media-body">
      <b>3. CoMoFusion: Fast and High-quality Fusion of Infrared and Visible Image with Consistency Model</b><br>
      Zhiming Meng, <strong><b>Hui Li*</b></strong>, Zeyang Zhang, Zhongwei Shen, Yunlong Yu, Xiaoning Song, Xiaojun Wu <br/>
      arXiv 2024 <br/>
      [<a href="https://arxiv.org/abs/2405.20764">arxiv</a>][<a href="https://github.com/ZhimingMeng/CoMoFusion">code</a>]
   </div>
</div>  

<div class="publication media paperhi">
   <img src="./images/preprint/2-preprint-lrr.png" height="120" width="200" class="papericon">
   <div class="media-body">
	   <b>2. Infrared and visible image fusion using Latent Low-Rank Representation</b><br>
   	<strong><b>Hui Li</b></strong>, Xiao-Jun Wu* <br/>
	   arXiv 2017 <br/>
   	[<a href="https://arxiv.org/abs/1804.08992">arxiv</a>][<a href="https://github.com/hli1221/imagefusion_Infrared_visible_latlrr">code</a>]
   </div>
</div>  

<div class="publication media paperhi">
   <img src="./images/preprint/1-preprint-mflrr.png" height="120" width="200" class="papericon">
   <div class="media-body">
	   <b>1. Multi-focus Noisy Image Fusion using Low-Rank Representation</b><br>
   	<strong><b>Hui Li</b></strong>, Xiao-Jun Wu*, Tariq Durrani <br/>
	   arXiv 2017 <br/>
   	[<a href="https://arxiv.org/abs/1804.09325">arxiv</a>][<a href="https://github.com/hli1221/imagefusion_noisy_lrr">code</a>]
   </div>
</div>


</div>



---
# Academic Services

Reviewer for:
KBS,ESWA,Information fusion,BSPC,Infrared Physics and Technology,Optics and Laser Technology,Biomedical Physics & Engineering Express,Image and Vision Computing,Neural Computing and Applications,Pattern Analysis and Applications,Machine Vision and Applications,Soft Computing,Optical and Quantum Electronics,Physics in Medicine and Biology,Frontiers in Physics,...






